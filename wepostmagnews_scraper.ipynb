{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wepostmagnews_scraper.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD2W5HIBid79",
        "colab_type": "text"
      },
      "source": [
        "# This notebook concerns the scraping of the news page of the website \"wepostmag.com\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8hQRfOja_dM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing libraries\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import requests as rq\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qL9sFDYAbDcb",
        "colab_type": "code",
        "outputId": "d0f1a56c-ee5b-4d53-de77-3062a17876f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "#opening a parser on the news section of the website (french)\n",
        "base_site='http://www.wepostmag.com/category/news/'\n",
        "response_base= rq.get(base_site)\n",
        "if (response_base.status_code != 200):\n",
        "  print('\\nthe french page cannot be opened')\n",
        "else:\n",
        "  L_articles=[]\n",
        "  all_links=[]\n",
        "  L_titles=[]\n",
        "  L_categories=[]\n",
        "  print('\\nthe french page was opened successfully')\n",
        "  soup=bs(response_base.content, 'html.parser')\n",
        "  #extracting links for more news pages\n",
        "  url=[]\n",
        "  for i in range(5):\n",
        "    page=base_site+'page/'+str(i+1)+'/'\n",
        "    url.append(page)\n",
        "  print('\\nfrench scraping in progress..')\n",
        "  for pg in url:\n",
        "    response_pg=rq.get(str(pg))\n",
        "    soup_pg=bs(response_pg.content,'html.parser')\n",
        "    print('\\nfrench scraping in progress....')\n",
        "    #extracting links of articles\n",
        "    links=[]\n",
        "    for i in soup_pg.find_all('div', class_='post-title'):\n",
        "      link=i.find('a').get('href')\n",
        "      links.append(link)\n",
        "      all_links.append(link)\n",
        "    #extracting content from articles\n",
        "    for i in links:\n",
        "      response_i=rq.get(str(i))\n",
        "      soup_i=bs(response_i.content,'html.parser')\n",
        "      #extracting the content of the article\n",
        "      ch=''\n",
        "      for p in soup_i.find('div', class_='post-content entry-content').find_all('p'):\n",
        "        ch=ch+p.text\n",
        "      ch=ch.replace('\\n','').replace('\\xa0','')\n",
        "      L_articles.append(ch)\n",
        "      #extracting the title of the article\n",
        "      L_titles.append(soup_i.find('h1', class_=\"entry-title\").text.replace('\\n',''))\n",
        "      #extracting the category of the article\n",
        "      cat=''\n",
        "      for a in soup_i.find('aside', class_='post-category post-detail-category').find_all('a', rel='category tag'):\n",
        "        cat=cat+ a.text + '/'\n",
        "      L_categories.append(cat)\n",
        "  print('\\nfrench scraping is done')\n",
        "  print('\\nyou have extracted ',str(len(L_articles)),'french articles')\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "the french page was opened successfully\n",
            "\n",
            "french scraping in progress..\n",
            "\n",
            "french scraping in progress....\n",
            "\n",
            "french scraping in progress....\n",
            "\n",
            "french scraping in progress....\n",
            "\n",
            "french scraping in progress....\n",
            "\n",
            "french scraping in progress....\n",
            "\n",
            "french scraping is done\n",
            "\n",
            "you have extracted  50 french articles\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbKjyTufZPru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creation of the french dataframe\n",
        "d = {'link': all_links,'titles' : L_titles ,'article': L_articles, 'type': L_categories}\n",
        "df = pd.DataFrame(data=d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdVPrEJyZYjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#saving the french dataframe\n",
        "df.to_csv('wepostmagnews-fr.csv', index=False, encoding='utf-16')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f3b1f5ae-5b99-4659-c975-e7b203a78d82",
        "id": "GYIB4QEveLVK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "#opening a parser on the news section of the website (arabic)\n",
        "base_site='http://www.wepostmag.com/ar/category/%D8%A3%D8%AE%D8%A8%D8%A7%D8%B1/'\n",
        "response_base= rq.get(base_site)\n",
        "if (response_base.status_code != 200):\n",
        "  print('\\nthe arabic page cannot be opened')\n",
        "else:\n",
        "  L_articles=[]\n",
        "  all_links=[]\n",
        "  L_titles=[]\n",
        "  L_categories=[]\n",
        "  print('\\nthe arabic page was opened successfully')\n",
        "  soup=bs(response_base.content, 'html.parser')\n",
        "  #extracting links for more news pages\n",
        "  url=[]\n",
        "  for i in range(5):\n",
        "    page=base_site+'page/'+str(i+1)+'/'\n",
        "    url.append(page)\n",
        "  print('\\narabic scraping in progress..')\n",
        "  for pg in url:\n",
        "    response_pg=rq.get(str(pg))\n",
        "    soup_pg=bs(response_pg.content,'html.parser')\n",
        "    print('\\narabic scraping in progress....')\n",
        "    #extracting links of articles\n",
        "    links=[]\n",
        "    for i in soup_pg.find_all('div', class_='post-title'):\n",
        "      link=i.find('a').get('href')\n",
        "      links.append(link)\n",
        "      all_links.append(link)\n",
        "    #extracting content from articles\n",
        "    for i in links:\n",
        "      response_i=rq.get(str(i))\n",
        "      soup_i=bs(response_i.content,'html.parser')\n",
        "      #extracting the content of the article\n",
        "      ch=''\n",
        "      for p in soup_i.find('div', class_='post-content entry-content').find_all('p'):\n",
        "        ch=ch+p.text\n",
        "      ch=ch.replace('\\n','').replace('\\xa0','')\n",
        "      L_articles.append(ch)\n",
        "      #extracting the title of the article\n",
        "      L_titles.append(soup_i.find('h1', class_=\"entry-title\").text.replace('\\n',''))\n",
        "      #extracting the category of the article\n",
        "      cat=''\n",
        "      for a in soup_i.find('aside', class_='post-category post-detail-category').find_all('a', rel='category tag'):\n",
        "        cat=cat+ a.text + '/'\n",
        "      L_categories.append(cat)\n",
        "  print('\\narabic scraping is done')\n",
        "  print('\\nyou have extracted ',str(len(L_articles)),'arabic articles')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "the arabic page was opened successfully\n",
            "\n",
            "arabic scraping in progress..\n",
            "\n",
            "arabic scraping in progress....\n",
            "\n",
            "arabic scraping in progress....\n",
            "\n",
            "arabic scraping in progress....\n",
            "\n",
            "arabic scraping in progress....\n",
            "\n",
            "arabic scraping in progress....\n",
            "\n",
            "arabic scraping is done\n",
            "\n",
            "you have extracted  50 arabic articles\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iEoX1_jygrcL",
        "colab": {}
      },
      "source": [
        "#creation of the arabic dataframe\n",
        "d = {'link': all_links,'titles' : L_titles ,'article': L_articles, 'type': L_categories}\n",
        "df_ar = pd.DataFrame(data=d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tUG_SbsagzJS",
        "colab": {}
      },
      "source": [
        "#saving the arabic dataframe\n",
        "df_ar.to_csv('wepostmagnews-ar.csv', index=False, encoding='utf-16')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}