{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "expressfm_scraper.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTDcmkwqRk5n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing libraries\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import requests as rq\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RP8A80lSGqFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#opening the french actualities section of the website\n",
        "base_site='https://www.radioexpressfm.com/actualites/'\n",
        "response_base= rq.get(base_site)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9qFchdTGvIV",
        "colab_type": "code",
        "outputId": "0d38c498-5de4-436e-eeba-9374e7a0d1a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "#scraping french news\n",
        "if (response_base.status_code!=200):\n",
        "  print('the french page cannot be opened')\n",
        "else:\n",
        "  print('page opened successfully')\n",
        "  soup=bs(response_base.content,'html.parser')\n",
        "  #getting more pages\n",
        "  url=[]\n",
        "  for i in range(4):\n",
        "    url.append(base_site+'page/'+str(i+1)+'/')\n",
        "  print('scraping in process..')\n",
        "  L_titles=[]\n",
        "  all_links=[]\n",
        "  L_articles=[]\n",
        "  L_categories=[]\n",
        "  for pg in url:\n",
        "    response_pg=rq.get(str(pg))\n",
        "    soup_pg=bs(response_pg.content,'html.parser')\n",
        "    links=[]\n",
        "    for item in soup_pg.find_all('a', class_=\"qt-text-shadow\"):\n",
        "      links.append(item.get('href'))\n",
        "      all_links.append(item.get('href'))\n",
        "      L_titles.append(item.text.replace('\\n',''))\n",
        "    print('scraping in process.....')\n",
        "    for i in links:\n",
        "      response_i=rq.get(str(i))\n",
        "      soup_i=bs(response_i.content,'html.parser')\n",
        "      ch=''\n",
        "      for p in soup_i.find('div', class_='qt-the-content').find_all('p'):\n",
        "        ch=ch+p.text\n",
        "      ch=ch.replace('\\n','').replace('\\xa0','')\n",
        "      L_articles.append(ch)\n",
        "      cat=''\n",
        "      for a in soup_i.find_all('a', rel='category tag'):\n",
        "        cat=cat+ ' '+a.text\n",
        "      L_categories.append(cat)\n",
        "  print('french scraping done')\n",
        "  print('you have extracted '+ str(len(L_titles)) + ' french articles')\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "page opened successfully\n",
            "scraping in process..\n",
            "scraping in process.....\n",
            "scraping in process.....\n",
            "scraping in process.....\n",
            "scraping in process.....\n",
            "french scraping done\n",
            "you have extracted 40 french articles\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBuEezfSMn9A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating the french dataframe\n",
        "d = {'link': all_links,'titles' : L_titles ,'article': L_articles ,'type': L_categories}\n",
        "df = pd.DataFrame(data=d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9uUE5lcNPs0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#saving the french dataframe\n",
        "df.to_csv('expressfm-fr.csv', index=False,encoding='UTF-16')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lrHmfj-1R3DC",
        "colab": {}
      },
      "source": [
        "#opening the arabic actualities section of the website\n",
        "base_site='https://www.radioexpressfm.com/ar/%d8%a7%d9%84%d8%a3%d8%ae%d8%a8%d8%a7%d8%b1/'\n",
        "response_base= rq.get(base_site)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e880c5b5-e138-470e-c68f-eb8839655342",
        "id": "OvoR4r7ASMGZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "#scraping arabic news\n",
        "if (response_base.status_code!=200):\n",
        "  print('the arabic page cannot be opened')\n",
        "else:\n",
        "  print('arabic page opened successfully')\n",
        "  soup=bs(response_base.content,'html.parser')\n",
        "  #getting more pages\n",
        "  url=[]\n",
        "  for i in range(4):\n",
        "    url.append(base_site+'page/'+str(i+1)+'/')\n",
        "  print('scraping in process..')\n",
        "  L_titles=[]\n",
        "  all_links=[]\n",
        "  L_articles=[]\n",
        "  L_categories=[]\n",
        "  for pg in url:\n",
        "    response_pg=rq.get(str(pg))\n",
        "    soup_pg=bs(response_pg.content,'html.parser')\n",
        "    links=[]\n",
        "    for item in soup_pg.find_all('a', class_=\"qt-text-shadow\"):\n",
        "      links.append(item.get('href'))\n",
        "      all_links.append(item.get('href'))\n",
        "      L_titles.append(item.text.replace('\\n',''))\n",
        "    print('scraping in process.....')\n",
        "    for i in links:\n",
        "      response_i=rq.get(str(i))\n",
        "      soup_i=bs(response_i.content,'html.parser')\n",
        "      ch=''\n",
        "      for p in soup_i.find('div', class_='qt-the-content').find_all('p'):\n",
        "        ch=ch+p.text\n",
        "      ch=ch.replace('\\n','').replace('\\xa0','')\n",
        "      L_articles.append(ch)\n",
        "      cat=''\n",
        "      for a in soup_i.find_all('a', rel='category tag'):\n",
        "        cat=cat+' '+ a.text\n",
        "      L_categories.append(cat)\n",
        "  print('arabic scraping done')\n",
        "  print('you have extracted '+ str(len(L_titles)) + ' arabic articles')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "arabic page opened successfully\n",
            "scraping in process..\n",
            "scraping in process.....\n",
            "scraping in process.....\n",
            "scraping in process.....\n",
            "scraping in process.....\n",
            "arabic scraping done\n",
            "you have extracted 40 arabic articles\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ja4c6fmjTtkW",
        "colab": {}
      },
      "source": [
        "#creating the arabic dataframe\n",
        "d = {'link': all_links,'titles' : L_titles ,'article': L_articles, 'type': L_categories}\n",
        "df_ar = pd.DataFrame(data=d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hBJG6ZrVTcG2",
        "colab": {}
      },
      "source": [
        "#saving the arabic dataframe\n",
        "df_ar.to_csv('expressfm-ar.csv', index=False,encoding='UTF-16')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}